{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style_transfer_based_signal_noising_for_CNN_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF5T89Dev7DH"
      },
      "source": [
        "# For tasks of classification, it is sometimes hard to avail real life data, as compared to numerically simulating the corresponding data. Therefore, instead of using spectogram of scarce real life data, we could style transfer the imperfections imparted in the spectrogram of real life data into numerically simulated data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihCeQ0oMLxxQ"
      },
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIgoAv8VOHoe"
      },
      "source": [
        "#Loadung the model vgg19 that will serve as the base model\n",
        "model=models.vgg19(pretrained=True).features\n",
        "#Assigning the GPU to the variable device\n",
        "device=torch.device( \"cuda\" if (torch.cuda.is_available()) else 'cpu')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVHCIiGnOLUc"
      },
      "source": [
        "#defing a function that will load the image and perform the required preprocessing and put it on the GPU\n",
        "def image_loader(path):\n",
        "    image=Image.open(path)\n",
        "    #defining the image transformation steps to be performed before feeding them to the model\n",
        "    loader=transforms.Compose([transforms.Resize((512,512)), transforms.ToTensor()])\n",
        "    #The preprocessing steps involves resizing the image and then converting it to a tensor\n",
        "    image=loader(image).unsqueeze(0)\n",
        "    return image.to(device,torch.float)\n",
        "\n",
        "#Loading the time-frequency scalogram of numerically generated signal and realistic signal\n",
        "original_image=image_loader('simulated_signal_scalogram.png')\n",
        "style_image=image_loader('realistic_signal_scalogram.png')\n",
        "\n",
        "#Creating the generated image from the original image\n",
        "generated_image=original_image.clone().requires_grad_(True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVkOagX1ONWX"
      },
      "source": [
        "#Defining a class that for the model\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG,self).__init__()\n",
        "        self.req_features= ['0','5','10','19','28'] \n",
        "        #Since we need only the 5 layers in the model so we will be dropping all the rest layers from the features of the model\n",
        "        self.model=models.vgg19(pretrained=True).features[:29] #model will contain the first 29 layers\n",
        "    \n",
        "   \n",
        "    #x holds the input tensor(image) that will be feeded to each layer\n",
        "    def forward(self,x):\n",
        "        #initialize an array that wil hold the activations from the chosen layers\n",
        "        features=[]\n",
        "        #Iterate over all the layers of the mode\n",
        "        for layer_num,layer in enumerate(self.model):\n",
        "            #activation of the layer will stored in x\n",
        "            x=layer(x)\n",
        "            #appending the activation of the selected layers and return the feature array\n",
        "            if (str(layer_num) in self.req_features):\n",
        "                features.append(x)\n",
        "                \n",
        "        return features"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3oCRnmcORzS"
      },
      "source": [
        "def calc_content_loss(gen_feat,orig_feat):\n",
        "    #calculating the content loss of each layer by calculating the MSE between the content and generated features and adding it to content loss\n",
        "    content_l=torch.mean((gen_feat-orig_feat)**2)\n",
        "    return content_l"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9F--BDBQPXC"
      },
      "source": [
        "def calc_style_loss(gen,style):\n",
        "    #Calculating the gram matrix for the style and the generated image\n",
        "    batch_size,channel,height,width=gen.shape\n",
        "\n",
        "    G=torch.mm(gen.view(channel,height*width),gen.view(channel,height*width).t())\n",
        "    A=torch.mm(style.view(channel,height*width),style.view(channel,height*width).t())\n",
        "        \n",
        "    #Calcultating the style loss of each layer by calculating the MSE between the gram matrix of the style image and the generated image and adding it to style loss\n",
        "    style_l=torch.mean((G-A)**2)\n",
        "    return style_l"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5tz-c5oQRzn"
      },
      "source": [
        "def calculate_loss(gen_features, orig_feautes, style_featues):\n",
        "    style_loss=content_loss=0\n",
        "    for gen,cont,style in zip(gen_features,orig_feautes,style_featues):\n",
        "        #extracting the dimensions from the generated image\n",
        "        content_loss+=calc_content_loss(gen,cont)\n",
        "        style_loss+=calc_style_loss(gen,style)\n",
        "    \n",
        "    #calculating the total loss of e th epoch\n",
        "    total_loss=alpha*content_loss + beta*style_loss \n",
        "    return total_loss"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruIIPsxgQUXp"
      },
      "source": [
        "#Load the model to the GPU\n",
        "model=VGG().to(device).eval() \n",
        "\n",
        "#initialize the paramerters required for fitting the model\n",
        "epoch=10\n",
        "lr=0.004\n",
        "alpha=8\n",
        "beta=70\n",
        "\n",
        "#using adam optimizer and it will update the generated image not the model parameter \n",
        "optimizer=optim.Adam([generated_image],lr=lr)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4McWWSwdQZr7",
        "outputId": "5ecac1fc-08ba-4a1e-d7bd-05895c719d09"
      },
      "source": [
        "#iterating for 100 times\n",
        "for e in range (epoch):\n",
        "    #extracting the features of generated, content and the original required for calculating the loss\n",
        "    gen_features=model(generated_image)\n",
        "    orig_feautes=model(original_image)\n",
        "    style_featues=model(style_image)\n",
        "    \n",
        "    #iterating over the activation of each layer and calculate the loss and add it to the content and the style loss\n",
        "    total_loss=calculate_loss(gen_features, orig_feautes, style_featues)\n",
        "    #optimize the pixel values of the generated image and backpropagate the loss\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    #print the image and save it after each 100 epoch\n",
        "    if(e):\n",
        "        print(total_loss)\n",
        "        \n",
        "        save_image(generated_image,\"gen.png\")    "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(6.5152e+08, grad_fn=<AddBackward0>)\n",
            "tensor(5.2016e+08, grad_fn=<AddBackward0>)\n",
            "tensor(4.5272e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.9060e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.4560e+08, grad_fn=<AddBackward0>)\n",
            "tensor(3.1276e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.8712e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.6618e+08, grad_fn=<AddBackward0>)\n",
            "tensor(2.4816e+08, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}